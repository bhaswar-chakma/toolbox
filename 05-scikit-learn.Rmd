# `scikit-learn`

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

```{python}
import sklearn
# check version
sklearn.__version__ 
```

## Linear Model

```{python}
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
```

```{python}
# data
olympic = pd.read_csv("https://raw.githubusercontent.com/sdrogers/fcmlcode/master/R/data/olympics/male100.csv", names = ["year","time"])
```

```{python}
olympic.head()
```

```{python}
olympic.tail()
```
```{python}
plt.scatter('year', 'time', data = olympic)
plt.show()
```

```{python}
# create an instance of a linear regression model where we will estimate the intercept
model = linear_model.LinearRegression(fit_intercept = True)
```

<span style="color: blue;">`scikit-learn` requires that the features (x) be a matrix and the response y be a one-dimension array.</span>

### Prepare X

```{python}
# Create an X matrix using the x values
x = olympic.year.values
x.shape
type(x)
```

```{python}
X = x.reshape([-1, 1]) # here - 1 means "I don't know how many..."
```

```{python, eval = FALSE}
# if you know the dimensions
X = x.reshape((28, 1))
```
```{python}
# Check the shape
print(X.shape)
```
Alternative? Try the following
```{python}
X2 = olympic[['year']]
X2.shape
```


### Prepare y
```{python}
y = olympic.time
y.shape
type(y)# fine! note the difference between year and time; we had to reshape year
```
### Fit 

```{python}
# Now fit the model
model.fit(X, y)

print(model.coef_) # coefficient
print(model.intercept_) # intercept
```
### Prediction

```{python}
# New X as np array
prediction_x = np.linspace(1900, 2000, 101)
# reshape it
prediction_x = prediction_x.reshape([-1, 1]) # recall -1 stands for "i don't know"
```
```{python}
model.predict(prediction_x)
```

### Scatter Plot: Actual vs Fitted

```{python}
plt.scatter(x, y)
plt.plot(prediction_x, model.predict(prediction_x), color = 'red')
plt.show()
```

### Residual Plot

```{python}
# find residuals
residuals = y - model.predict(X)
np.mean(residuals) # check mean
```


```{python}
plt.hist(residuals)
plt.show()
plt.plot(x, residuals, "o")
plt.show()
```


## Train-Test

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import linear_model, preprocessing, model_selection
from sklearn.model_selection import train_test_split, cross_val_score
```



`train_test_split()` takes a list of arrays and splits each array into two arrays (a training set and a test set) by randomly selecting rows or values.

### Example

```{python}
# x is our predictor matrix
X = np.arange(20).reshape((2, -1)).T
print(X)
```

```{python}
# y is a numeric output - for regression methods
y = np.arange(10)
print(y)
```

```{python}
# z is a categorical output - for classification methods
z = np.array([0,0,0,0,0,1,1,1,1,1])
print(z)
```


We can use `train_test_split()` on each array **individually**. 

What happens?

```{python}
train_test_split(X, test_size = 1/4, random_state = 1)
```

```{python}
type(train_test_split(X, test_size = 1/4, random_state = 1))
```

Store them 

```{python}
X_train, X_test = train_test_split(X, test_size = 1/4, random_state = 1)
print(X_train)
print(X_test)
```

```{python}
y_train, y_test = train_test_split(y, test_size = 1/4, random_state = 1)
print(y_train)
print(y_test)
```

We can also apply it to multiple arrays **simultaneously**.

```{python}
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1/4, random_state = 1)
print(X_train)
print(X_test)
print(y_train)
print(y_test)
```

If you have a **categorical** variable, the `stratify` argument ensures 
that you'll get an appropriate number of each category in the resulting split.
For this purpose, we previously created `z`.

```{python}
X_train, X_test, z_train, z_test = train_test_split(
  X, z, test_size = 1/4, random_state = 1, stratify = z
)
```

```{python}
print(X_train)
print(X_test)
print(z_train)
print(z_test)
```

### Another Example

```{python}
# Example data: ironslag
iron = pd.read_csv('https://raw.githubusercontent.com/bhaswar-chakma/toolbox/main/data/ironslag.csv')
```

```{python}
iron.head()
iron.shape
```


*Magnetic test is cheaper; chemical test is more accurate.Can we use the magnetic test to predict the chemical test result?*

- `X` = magnetic test result

- `y` = chemical test

```{python}
plt.scatter(iron.magnetic, iron.chemical)
```


**Create a hold-out set using train-test split**

```{python}
train, test = train_test_split(
  iron, test_size = 1/5, random_state = 1
)
```
```{python}
train.shape
train.head()
```

```{python}
test.shape
test.head()
```


```{python}
plt.scatter(train.magnetic, train.chemical)
```


**Use only the training data to try out possible models**

```{python}
# sklearn requires our predictor variables to be in a two dimensional array
# reshape to have 1 column
# the -1 in reshape means I don't want to figure out all the necessary dimensions
# i want 1 column, and numpy, you figure out how many rows I need
X = train.magnetic.values.reshape(-1,1)
X.shape
```

```{python}
y = train.chemical.values
y.shape
```
```{python}
np.corrcoef(train.magnetic.values, train.chemical.values)
```

```{python}
# r-squared
np.corrcoef(train.magnetic.values, train.chemical.values)[0,1] ** 2
```
**Fit a linear model between x and y**

```{python}
linear = linear_model.LinearRegression()
linear.fit(X, y)
```

`linear.score()` is the $R^2$ value.

```{python}
# linear.score is the R^2 value
# how much error is reduced from no model (variance or MSE)
# vs having the regression model
linear.score(X, y)
```
```{python}
x_predict = np.arange(10, 40).reshape(-1,1) # values to be used for prediction
lin_y_hat = linear.predict(x_predict) # use the values and predict
```
```{python}
plt.scatter(X, y)
plt.plot(x_predict, lin_y_hat, c = 'red')
```


## Cross Validation

**Linear Model**

```{python}
# shuffle split says 'shuffle the data' and split it into 5 equal parts
cv = model_selection.ShuffleSplit(n_splits = 5, test_size = 0.3, random_state=0)
cv_linear = model_selection.cross_val_score(linear, X, y, cv = cv)
print(cv_linear)
print(np.mean(cv_linear))
```

**Polynomial Fit - Quadratic**

```{python}
# preprocessing polynomial features creates a polynomial based on X
quad = preprocessing.PolynomialFeatures(2)
quadX = quad.fit_transform(X)
quad_model = linear_model.LinearRegression()
quad_model.fit(quadX, y)
cv_quad = model_selection.cross_val_score(quad_model, quadX, y, cv = cv)
print(cv_quad)
print(np.mean(cv_quad))
```

**Cubic Fit**

```{python}
cube = preprocessing.PolynomialFeatures(3)
cubeX = cube.fit_transform(X)
cube_model = linear_model.LinearRegression()
cube_model.fit(cubeX, y)
cv_cube = model_selection.cross_val_score(cube_model, cubeX, y, cv = cv)
print(cv_cube)
print(np.mean(cv_cube))
```

**Y ~ log X model**

```{python}
log_transform = preprocessing.FunctionTransformer(np.log)
logX = log_transform.fit_transform(X)
logX_model = linear_model.LinearRegression()
logX_model.fit(logX, y)
cv_logX = model_selection.cross_val_score(logX_model, logX, y, cv = cv)
print(cv_logX)
print(np.mean(cv_logX))
```

